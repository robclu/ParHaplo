%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% witseiepaper-2005.tex
%
%                       Ken Nixon (12 October 2005)
%
%                       Sample Paper for ELEN417/455 2005
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,twocolumn]{witseiepaper}

%
% All KJN's macros and goodies (some shameless borrowing from SPL)
\usepackage{KJN}
\usepackage{textcomp}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm}

\newcommand{\K}{\textit{k }}
\newcommand{\M}{\textit{m }}
\newcommand{\N}{\textit{n }}
\newcommand{\D}{\textit{d}}
\newcommand{\PP}{\textit{p}}
\newcommand{\QQ}{\textit{q}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF Info %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ifpdf
\pdfinfo{
/Title (Parallel Individual Haplotyping Assembly : Xeon Phi vs. Nvidia K20x )
/Author (Robert Clucas)
/CreationDate (D:200309251200)
/ModDate (D:200510121530)
/Subject (ELEN4002 Laboratory Project, 2015)
/Keywords (Haplotyping, GPU, Branch, Bound, Simplex)
}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE ETC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{ Parallelizing the Individual Haplotying Assembly Problem }

\author{Robert J. clucas
\thanks{School of Electrical \& Information Engineering, University of the
Witwatersrand, Private Bag 3, 2050, Johannesburg, South Africa}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\abstract{}

\keywords{Brach, Bound, GPU, Haplotyping, Simplex}

\maketitle
\thispagestyle{empty}\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{INTRODUCTION}

It is commonly accepted that all humans share $\mathtt{\sim}$99$\%$ of the same DNA, however, small variations 
cause human beings to have different physical traits. Single nucleotide polymorphisms (SNPs), which are
variations of a single DNA base from one individual to another, are believed to be able to address
genetic differences. For diploid organisms, which have pairs of chromosomes, a \textit{haplotype} is a 
sequence of SNPs in each copy of a pair of chromosomes. A \textit{genotype} describes the conflated data of the
haplotypes on a pair of chromosomes. Haplotypes are believed to contain more generic information than
genotypes \cite{stephens:2001}, however, obtaining haplotypes correctly is a difficult problem, which is 
broken into two subdomains: haplotype assembly and haplotype inference. \\
Haplotype inference uses the genotype of a set of individuals. The genotype data tells the status of each
allele at a position, but does not distinguish which copy of the chromosome the allele came from.
This negative aspects of this approach are that it cannot distinguish rare and novel SNPs \cite{he:2010}, 
and there is no way of knowing if the inferred haplotype is completely correct. \\
Individual haplotype assembly uses fragments of sequences generated by sequencing technology to determine
haplotypes. The fragments of a sequence come from the two copies of an individual's chromosome, the goal of the
individual haplotyping problem is to correctly determine two haplotypes, where each haplotype corresponds to
one of the two copies of the chromosome. \\
The haplotype assembly problem was proven to be NP-Hard \cite{lippert:2002}. The algorithms used to solve the
problem are thus computationally complex and until recently, there was no practical exact algorithm to solve 
the problem using minimum error correction (MEC) \cite{bonizzoni:2003},
However, recently an exact solution was proposed by \cite{chen:2013} which is capable of solving the MEC 
problem exactly, and can thus correctly infer all haplotypes from the fragment sequences. Due the NP-Hardness 
of the problem, the algorithm results in long run times - in the range of days for chromosomes with high
errors rates. Using a parallel implementation of any of the proposed solutions could reduce the long run
times, allowing useful haplotype information to be quickly inferred from the available datasets, having
positive effects in fields such as drug discovery, prediction of diseases, and variations in gene
expressions, to name a few. \\
Parallel programming makes use of devices which have numerous cores, and uses these cores to execute a single
instruction on multiple data (SIMD). The effectiveness of parallel programming is dependant on the nature of 
the problem, as per Amdahl's law. While using multiple processors can potentially allow for large performance 
increases, in practise this is difficult to achieve due to additional copliexities which are introduces with 
parallel capable devices. The main difficulties are the communication between the multiple cores, and the 
managemet of memory. Threads are usually created in blocks, and the memory heirachy allows threads within the 
same block to share specific memory. All threads have access to a global memory space, however, access to this 
memory space is slower, decreasing performance. Furthermore, race conditions, where multiple threads attempt to 
access memory at the same location simultaneously, can cause undefined behaviour and cause inaccurate results. \\
With the increasing popularity of General-Purpose GPU (GPGPU) programming, however, API's like CUDA and OpenCL
wich provide access to GPUs through simple function calls in C and C++ program, have made parallel programming
easier on GPUs. Intel have also made a move towards parallel programming, introducing the Xeon Phi, which, like 
a GPU, has numerous cores which can operate on data in parallel. The Xeon Phi is also programmed using an API 
\cite{intel:2013}, however, far more extensive use is made of compiler directives than with CUDA or OpenCL, 
with API functions being provided to gain access to parallel variables such as the thread index and number of 
threads which are runnning in parallel. \\
The contibution of this paper is to idenfiy one of the proposed algorithms for solving the HA problem, and then 
to identify components of the selected algorithm which would be suited for parallelism before proposing a 
possible parallel implementation of the potentially parallelizable components. \\
The remainder of this paper is structured as follows. Section~\ref{sec:hap} describles the HA problem, and gives 
an overview of some of the proposed algorithms for solving it.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{HAPLOTYPE ASSEMBLY PROBLEM} \label{sec:hap}

This section will provide a brief overview of the haplotype assembly (HA) problem, and define the
notation used through the rest of the paper. The input to the problem is a set of reads from a given genome
sequence, where each read contains fragments from each of the two chromosomes which make up the genome
sequence. These characters of a read consist of elements from a \textit{ternary string}, where a ternary
string has characters from the set \{0, 1, -\}. A value of 0 refers to the major allele
at a site, a value of 1 to the minor allele, and a value of - to the lack of a read at the site, and is
referred to as a \textit{gap}. These reads are then combined to form a matrix, M, where each row of the 
matrix corresponds to a read. \\
Each column of the matrix is known as an SNP site. At each site, the data could be accurate, missing, or have
error. The goal of the haplotype assembly problem is to determine a haplotype, H = \{\textit{h,h'}\}
from the matrix. The following terminology will be used to refer to properties of the matrix and the
fragments. \\
For the input matrix M, the number of fragments is denoted by \textit{m}, which is the number of rows in M.
The number of SNP sites is denoted by \textit{n}, which is the number of columns in M, while the j$^{th}$ 
site of the i$^{th}$ fragment is given by \textit{f$_{ij}$}. Furthermore, two fragments are said to 
conflict if the following conditions are true:
\begin{itemize}
    \item{f$_{ik}$ $\neq$ f$_{jk}$ \textbf{and} f$_{ik}$ $\neq$ '-' \textbf{and} f$_{jk}$ $\neq$ '-' }
\end{itemize}
Essentially this means that for two fragments i and j, if at an SNP site k, the reads do not have error and are 
not gaps, the reads have different values (fragment i has a value 0 at site k, while fragment j has a value 
1 at site k, or vice versa). \\
Following the notion of a conflict, the \textit{distance} between two fragments (or ternary strings) is
denoted by d( f$_i$, f$_j$ ) is the total number of positions for which the two fragments f$_i$ and f$_j$
conflict. Furthermore, to understand some of the problem formulations from the fragment data, it is useful to
define a \textit{conflict graph} \cite{lancia:2001} G = \{ V, E \}, where V corresponds to a fragment, and E
corresponds to and edge between two fragments if they conflict. If the matrix M contains no errors, then none
of the fragments from the same chromosome will conflict and G will be bipartate. Fragments from the same
chromosome may conflict. However, if there are errors (as is usually the case) in M, G will not be
bipartate. The haplotype assembly problem then requires the correction of G from a non-bipartate 
graph to a bipartate graph. There are numerous methods for solving the problem:

\begin{itemize}
    \item{ \textbf{Minimum Fragment Removal (MFR) :} This invloves removing the least number of fragments 
            from the input data such that the resultant graph G is bipartate. It is shown in 
            \cite{lancia:2001} that this can be solved in polynomial time.
        }
\item{ \textbf{Weighted Minimum Edge Removal (WMER) \cite{aguiar:2012} :} This method is a recent method,
        which requires determining the minimum number of edges to remove such that removal of the edges
        results in G being bipartate.
    }
\item{ \textbf{Longest Haplotype Reconstruction (LHR) :} This requires finding a set of fragments which,
        when they are removed from M, result in G being bipartate and the length of the resultant 
        haplotypes being maximized \cite{schwartz:2010}. 
    }
\item{ \textbf{Minimum Error Correction (MEC) :} This method involves correcting the minimum number of 
        elements (sites for all fragments) in the matrix M which allow the graph G to be bipartate. 
        Although being the most complex method, it is the most widely used method as it provides the 
        highest accuracy rates. Only recently has an exact algorith for the MEC problem formulation been
        proposed which can provide an exact solution for all cases.
    }
\end{itemize}

Due to the accuracy and more extensive previous work, the MEC method for solving the problem was decided upon
as the method for which a parallel implementation will be proposed.

\subsection{ Minimum Error Correction Implementations } \label{sec:mecimp}

Much research has been done on solving the MEC formulation of the HA problem. The first exact algorithm for
solving the problem was a branch and bound algorithm proposed by \cite{wang:2005}. The algorithm creates a
tree which covers the search space of all possible corrections. The tree is then traversed to find the best
solution. They use an upper bound for when branching that allows branches to be abandoned when a better
solution than the current best cannot result from further exploration of the branch, thus improving
performance. However, this exact method has time complexity of O(2$^{\textit{m}}$), where \M is the
number of fragments in the input data, and hence cannot produce solutions for large problem sizes. They also 
provide a heuristic \textit{genetic algorithm} (GA) to improve the computational time, which only requires 
run times up to three orders of magnitude faster than the branch and bound implementation. While the GA
implementation gives very similar results to the branch and bound implementation, it is slightly worse. \\
A dynamic programming solution was proposed by \cite{xie:2008} which addresses the run time problem for large
input sizes. The algorithm has time complexity of O(\textit{mk}3$^{\textit{k}}$ + \textit{mlogm} +
\textit{mk}), where \K is the maximum number of SNP sites a fragment covers. In practice \K is usually small,
and results were shown small \K (less than 100). The proposed dynamic programming solution had significant run
time improvements over the solution proposed by \cite{wang:2005}. However, for larger \K values, the 
algorithm cannot solve the MEC case of the HA problem in a feasible amount of time. \\
More recently, \cite{chen:2013} proposed an exact algorithm for solving the MEC problem. The proposed
algorithm is the currently the only algorithm which can solve the HA problem for both the homozygous (the 
alleles at a site are identical) and hetrozygous (the alleles at an SNP site are different) case. Most other 
work assumes that the input fragment data is hetrozygous, which, while true for most of the SNP sites in the
input matrix, is normally false for a small number of the SNP sites. The exact solution removes unnecessary
data from the matrix, and partitions the matrix into smaller sub-matrices. The problem is then formulated as
an \textit{integer linear programming} (ILP) problem and solved as an optimization problem. The implementation was
run using an Intel i7-3960X CPU, and the HuRef dataset required 15 days to solve for the general case, and 24h
for the all-hetrozygous case. Heuristics methods are also proposed which speed up computation time by up to 15
times. However, the results are only shown for smaller input sizes, and the heuristic methods do not always
determine the optimal solution. Furthermore, the solutions for the general case show lower MEC scores, meaning
that the all-hetrozygous assumption is not always valid. 

\subsection{Parallelization}

Of the MEC implementations discussed in Section~\ref{sec:mecimp}, the branch and bound solution proposed by
\cite{wang:2005}, and the ILP formulation of the problem proposed by \cite{chen:2013} have the most potential 
for a parallel implementation. While there are other methods for solving ILP problems \cite{galli:2014} the 
most common methods for solving ILP problems are:
\begin{itemize}
\item{ \textbf{Branch and bound :} The search space is divided into sub spaces, each of which is explored
        for the optimal solution. Bounds are enforced to ensure that sections of the sub spaces for which
    an optimal solution will not be found, will not be searched.
}
\item{ \textbf{Branch and cut :} The problem is first solved without the integer constraints to find the
    optimal solution. Cutting planes are then used to divide the search space and then branch and bound is
    applied.
}        
\end{itemize}
Thus both the MEC methods mentioned above can be solved using a branch and bound algorithm. The branch and cut
algorithm involves using a branch and bound method as well as the simplex method, and is thus more complex,
hence the choice was made to use the branch and bound algorithm. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{BRANCH AND BOUND} \label{sec:bnb}

The branch and bound algorithm can be parallelized in multiple ways \cite{crainic:2006}. Either specific,
computationally difficult functions can be accelerated by a parallel implementation, for example matrix
inversion, or the entire tree can be search in parallel, or a combination of both can be employed. Using tree
based approaches will require communication between each of the processes which searches a branch, as the
branch solution will need to be compared against the solutions of the other branches. This becomes a
bottleneck for both CPU and GPU implementations as groups of threads are assigned local memory, for which
access is much faster than the global memory space. There have been numerous parallel branch and bound methods
proposed for both the CPU and GPU which deal with these problems in different ways.

\subsection{ CPU Implementations } \label{ssec:cpuimp}

The ALPS framework \cite{xu:2005} is written in C++ and provides a parallel CPU implementation of the branch
and bound algorithm. The framework is tested on the knapsack problem \cite{kedia:2005}, which is an ILP problem 
and is known to be NP-hard. The speedup achieved is near linear in the number of nodes when the number of 
nodes is small, but diminishes as more nodes are added due to the amount of communication between the nodes. 
A speedup of 8 times is achieved for 8 nodes, and 26 times for 32 nodes. It is likely that for the HA problem, 
the number of nodes could be extremely large in which case this methods will be insufficient. \\
The MALLBA framework \cite{alba:2002} is also written in C++ and provides a parallel branch and bound
algorithm. It labels nodes as \textit{master} or \textit{slave} nodes, which defines the type of work that the
nodes do. The framework also provides heuristic methods to solve the problems more efficiently, however, the
accuracy of the results is less, which while acceptable for many applications, is not acceptable for the HA
problems. The performance results are similar to the ALPS framework, where near linear speed up is achieved for
a small number of additional nodes, but levels off for larger numbers of nodes.

\subsection{ GPU Implementations } \label{ssec:gpuimp}

A hetrogeneous CPU-GPU implementation was proposed by \cite{bouk:2012} and was applied to the knapsack 
problem. Due to the overhead of transferring data from the CPU to the GPU before computation, the model only 
uses GPUs when the tree has a large number of nodes ($>$ 5000), otherwise the CPUs are used Furthermore, the 
tree is built using a \textit{breadth first} strategy to favour the parallel nature of the GPUs. Both the
branching and bounding steps can be performed on the CPU or GPU. If the number of nodes is sufficiently large
such that full occupancy of the GPU is ensured, then for each iteration of the algorithm the GPU does the
branching and bounding on a list of nodes, eliminates nodes for which an optimal solution cannot be found, and
return the list back to the CPU. This process continues until convergence. This regular communication between
the CPU and GPU is expensive, which lessens the speedup achieved by the implementation. However, a speedup of 
up to 9.27 times was achieved for larger problem sizes, validating the feasibility of the branch and bound 
algorithm for parallel implementation. It must be noted that the algorithm was developed for Nvidia's Fermi
architecture, which does not support dynamic parallelism \cite{nvidia:2012}, hence results in the vast CPU-GPU 
communication. The newer Kepler architecture, however, does support dynamic parallelism. \\ 
In \cite{melab:2012, chakroun:2012}. algorithms are proposed which target the bounding operation for parallelism,
as well as dealing with the problem of thread divergence when using GPUs for branch and bound, which comes
from the position of the nodes in the tree, and the need to for branches to communicate with other branches to
compare solutions. Depending on the problem to which branch and bound is applied, the tree structure can be
irregular which makes the entire tree search unsuited for GPUs not supporting recursion and dynamic 
parallelism, as was the case when the algorithm was proposed - hence the focus on only he bounding 
operation. The algorithm was applied to the Flow-Shop scheduling problem, for which it is shown that
$\mathit{\sim}$98$\%$ of the computation is spent on the bound operation. Speed ups of up to
100 times were achieved by the GPU implementation over a multi-threaded CPU implementation. However, this 
kind of speedup will only be seen in problems where the calculation of the bounds is the bottleneck. 
Nevertheless, even if significantly less time is spent calculating the bounds, the speed up should still be
considerable.\\
The algorithms of \cite{melab:2012, chakroun:2012} are extended by \cite{chakroun:2013} to 
include not only parallelization of the bounding operator, but also the branching and pruning operators. This
essentially uses the GPU for doing all the searching of the subspaces. However, the CPU is still used for the
between each iteration as each GPU thread can only determine the solution of its first child nodes, rather 
than all child nodes until the leaves of the tree. Again this is due to the limitations of the Fermi 
architecture not supporting dynamic parallelism, despite this hardware limitation, the algorithm is still able
to achieve a speed up of up to 166 times over a multi-threaded CPU implementation, for large problem sizes.

\subsection{ Potential Improvements }

Sections~\ref{ssec:cpuimp} and \ref{ssec:gpuimp} above provide evidence of the feasibility of taking a
parallel approach to the branch and bound algorithms, and hence the feasibility of solving the HA problem with
a parallel implementation. Some aspects of the related work have significant performance implications on the
parallel implementation, such as:

\begin{itemize}
\item{ Using multiple CPUs does not scale effectively, and hence the relative performance increase for
    adding an additional CPU decreases for each CPU which is added
}
\item{ The amount of communication between the CPU and the GPU is large, and is required on each
        iteration, which decreases the performance of the algorithm
}
\item{ The structure of the tree depends on the problem - one problem may ensure a balanced tree while
        another may map to an unbalanced tree - which makes a GPU difficult due to its data-parallel
        nature
}
\end{itemize}

The above factors come from the limitations of the parallel hardware available at the time. Due to the lack of
recursive ability for the Fermi architecture used, each time solutions need to be compared for each of the
branches, and then to select the most relevant nodes from the list of searchable nodes, all results from the
GPU kernels needed to be passed back to the CPU since the GPU cannot invoke kernels itself. Furthermore, the 
problem of an unbalanced tree was significant for the same reason. However, the newer Kepler architecture does
allow kernels to call kernels, which should reduce the CPU-GPU communication and be able to handle unbalanced
trees as the kernel can itself determine the best searchable nodes. The Intel Xeon Phi also supports
recursion, and is thus also a good candidate for this approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{ PROBLEM DESCRIPTION } \label{sec:probdes}

Considering the potential performance of a parallel from the existing research, as well as the abiliy of the
algorithm presented in \cite{chen:2013} to solve both the homozygous and hetrozygous case of the HA problem,
this was the algorithm whis was chosen for the parallelization. This section will describe the agorithm
presented in \cite{chen:2013} which is the formulation of the HA problem as an ILP problem. Using the
terminology presented in \ref{sec:hap}, the Hamming distance between two fragments j and k is number of SNP 
sites at which the fragments conflict, and is denoted by \D($f_j$, f$_k$), which is used to determine the
MEC score of a solution H = \{ h, h' \}, which is given by
\begin{equation}
\textrm{MEC$_{score}$} = \sum_{i = 1}^{m}{ min\{ \D(f_i, h), \D(f_i, h') \} }
\end{equation}
Where \M is again the number of rows in the input matrix M. The MEC score is \textit{optimal} if it is the
minimum possible score. Some assumptions are made for the input matrix as per \cite{chen:2013}, namely
\begin{itemize}
\item{ No row of the input matrix M is useless - at least one entry in the row is a 1 or 0 
}
\item{ No column of M in monotone - the column must have at least one 0 and one 1
}
\item{ No column of M contains more 1's than 0's - if this is not the case the values are all flipped, which
    does not change the solution
}
\end{itemize}
The input matrix M first undergoes preprocessing to ensure that the above conditions are met. It must be noted
that the preprocessing does not affect the solution in any way. The preprocessing functions are:
\begin{itemize}
\item{ \textbf{Block reduction :} Splits the matrix into disjoint sets such that all reads start and end
        in the same block
}
\item{ \textbf{Block decomposition :} This process takes reduced blocks and eliminates redundancy, resulting
    in smaller problems 
}
\item{ \textbf{Singleton removal :} A singleton is a row for which the start and end positions of the fragment
    are the same - i,e the fragment has only one element. Since singletons do not affect the MEC score,
    they can be removed.
}
\item{ \textbf{Duplicate removal :} Rows and columns which are the same are merged into a single row or
        column, and the multiplicity of the row or column is recorded
}
\end{itemize}

From the reduced blocks the ILP problem is formulated for the all-hetrozygous and the general case. Despite
being more complicated, only the general case formulation will be shown. The justification for this is that  
the results presented in \cite{chen:2013} show that this is the case for which the optimal MEC scores are 
obtained, furthermore the all-hetrozygous case assumed that there are no errors in the input matrix, which
in not true for all input cases

\subsection{ ILP Formulation } \label{ssec:ilpform}

For integers \PP,\ \QQ\ $\in$ $\mathbb{Z}$, where 1 $\le$ \PP\ $\le$ \M, and 1 $\le$ \QQ\ $\le$ \N, \PP\ is the 
index of the fragment (or row) in the input matrix M, and \QQ\ is the index of SNP (or column) in the input 
matrix. The multiplicity of the $j^{th}$ column of M is denoted by $c_j$, while the multiplicity of the
$i^{th}$ row of M is denoted by $c_j$. The following binary variables are introduced, $y_i$, which has a value of 1
if \D($f_i$,h) $<$ \D($f_i$, h'), otherwise has a value of 0 (when \D($f_i$, h') $<$ \D($f_j$,h)). Informally,
if fragment $i$ is part of $h$ then it has a value of 1, otherwise it has a value of 0, $x_j$, which has a 
value of 1 if the $j^{th}$ bit of $h$ is 1, otherwise has a value of 0 of the $j^{th}$ bit of $h$ is 0, and
lastly $z_j$, which has a value of 1 if the $j^{th}$ bit of h' is 1, otherwise has a value of 0 if the
$j^{th}$ bit of h' is 0. Lastly $J_{i,0}$ ($J_{i,1}$) are the sets of integers $j \in \{1,2,...,q\}$ for which 
the $i^{th}$ value in column $j$ is a 0 (1), and the column is intrinsically hetrozygous. Essentially for each
row there are two sets, where each a set lists the intrinsically hetrozygous columns which have either values 
of 0 or 1. DO J-BAR \\
Using the above mentioned variables, an integer programming formulation is possible, however, non-linear 
terms in the form of $y_ix_j$ and $y_iz_j$ arise, which cannot be solved using ILP techniques. To overcome
this problem, \cite{chen:2013} defines variables $t_{i,j}$ for $y_ix_j$ and $u_{i,j}$ for $y_iz_j$ and impose
constraints on the variables which ensure linearity (the constraints are shown in the final formulation of the
problem). Using these variables, the final ILP formulation of the HA problem for the general case is 
\begin{equation*}
\begin{split}
    \textrm{Minimize} 
    &\ \ \ \ \sum_{i = 1}^{p}{w_i} \sum_{j \in J_{i, 0} }^{}{c_j(1 - x_j - y_i + 2t_{i,j})}     \\
    &+ \sum_{i = 1}^{p}{w_i} \sum_{j \in J_{i, 1}}^{}{c_j(y_i + x_j - 2t_{i,j})}                \\
    &+ \sum_{i = 1}^{p}{w_i} \sum_{j \in J_{i, 0}}^{}{c_j(z_j + t_{i,j} - u_{i,j})}             \\
    &+ \sum_{i = 1}^{p}{w_i} \sum_{j \in J_{i, 1}}^{}{c_j(1 - z_j - t_{i,j} + u_{i,j})}
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
    \textrm{Subject to} 
    &\ \ \ \ \vee_{1 \le i \le p} y_i \in \{0, 1\}                                              \\
    &\ \ \ \ \vee_{1 \le j \le q} x_j \in \{0, 1\}                                              \\
    &\ \ \ \ \vee_{1 \le i \le p} \vee_{1 \le j \le p} \ \ t_{i,j} \in \{0, 1\}                 \\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t_{i,j} \le y_i                        \\ 
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t_{i,j} \le x_j                        \\ 
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t_{i,j} \ge y_i + x_j + 1              \\
    &\ \ \ \ \vee_{1 \le i \le p} \vee_{j \in J_{i,0} \textrm{U} J_{i,1}} \ \ u_{i,j} \le y_i   \\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ u_{i,j} \le z_j            \\
    &\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ u_{i,j} \ge y_i + z_j - 1 
    \end{split}
\end{equation*}

\subsection{Computation Procedure}

Using the information provided by Section~\ref{sec:probdes} and Section~\ref{ssec:ilpform}, the general
procedure for solving the MEC HA problem using ILP can be defined as
\begin{algorithm}[t*]\label{alg:proc}
    \small
    \caption{Procedure for solving the MEC HA problem using ILP}
\textbf{Step 1:} Determine intrinsically hetrozygous and homozygous columns in M                        \\
\textbf{Step 2:} Perform block reduction on M                                                           \\
\textbf{Step 3:} Perform block decomposition on intrinsically hetrozygous columns in M                 	\\
\textbf{Step 4:} Remove singleton rows from M                                                           \\
\textbf{Step 5:} Remove duplicate columns in M								\\
\textbf{Step 6:} Solve ILP formulation using branch and bound algorithm 
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{ PARALLEL PRE-PROCESSING FOR MEC HA }

Observing Algorithm~\ref{alg:proc}, as well as referring to \cite{chen:2013} for the exact definitions of the
operations, there are numerous areas to which parallelism can be applied.

\subsection{ Determining Intrinsically Hetrozygous Columns}

Determining instrinsically hetrozygous columns in M requires comparing counting the number of rows at each of
the j columns which are 0 and which are 1, and then comparing the value. A parallel implementation would
perform this operation for all \textit{n} columns at once if the GPU was used. However, for small \textit{n}
the overhead of transferring the data to the GPU for computation would result in worse performance. In this
case, a better approach would be to used the vectorized operations available to modern CPUs to perform the
operations on multiple columns at a time, which could result in up to 8 times speedup depending on the CPU.

\subsection{Singleton Removal}

Finding singletons is a simple task as it only requires determining if there is more than one element in a
row. Again, vectorized operations could be used, however, this time one the GPU, which could use \textit{n}
threads for each row. Each thread checks if its corresponding entry in the row is present, and if it is,
increments a counter in shared memory. If the value of the counter is $>$ 1, then the row in not singular.
This would have time complexity of O(1) , compared to O(\textit{n}) using the implementation of
\cite{chen:2013}.

\subsection{Duplicate Removal}

For the implementation used by \cite{chen:2013}, each row needs to be compared with each other row to
determine if the rows are identical, and the same procedure is required for the columns. This can be done with
time complexity of O(Llogk), where L in the length of the reads and k is the number of reads. There are
numerous ways in which this operation can be parallelized.

\subsubsection{Simultaneous column and row search} This would use the GPU to create threads that perform the
column and row comparisons at the same time. Furthermore, multiple columns and rows can be done at the same
tome. For example, the $i^{th}$ and $z^{th}$ rows can be compared to all the other rows at the same time. This
would allow a speed up of  
\begin{equation}
    \textrm{Speed up} = N * \frac{\textrm{CPU frequency}}{\textrm{GPU frequency}} - \alpha B
\end{equation}
where N is the number of threads that are used simultaneously, $\alpha B$ is the proportion of the computation
which is spent transferring the data between teh CPU and GPU. Specifically $\alpha$ is a scaling parameter.

\subsubsection{Simultaneous row and column comparison} This implementation would compare the $i^{th}$ row or
column to multiple other rows or columns at the same time (for example to the
\{a$^{th}$,b$^{th}$,..j$^{th}$,k$^{th}$,...z$^{th}$\} rows or column at the same time). This would be much
faster provided the parallel device has a sufficient number of threads. For very large input sizes there will
be a serial aspect as there will not be enough threads. Nevertheless, for each of the iterations, the time
complexity would be O(L + K), assuming there were 2*L*K threads available at each iteration, where L is again
the length of the reads and K is the number of reads (this is because each row would spawn L*K threads to
cover the whole matrix on each iteration, and each column would spawn the same.

\section{PARALLEL BRANCH AND BOUND FOR MEC HA }

Considering the related work presented in Section~\ref{sec:bnb}, a similar approach to those presented in
\cite{melab:2012, chakroun:2012, chakroun:2013} will be taken, however, an attempt will be made to limit the
number of transactions between the CPU and GPU, preferrring rather to perform as much calculation on the GPU
as possible. Furthermore, the implementation will not attempt to balance the tree since the parallel devices
which will be used are capable of recursion, allowing branches to be explared until either an optimal or
infeasible solution is reached. Simply splitting the problem space into many sub problems will not be
sufficient for large performance increases. The hard blocks of the HuRef dataset took up to 12 hours on a
multi-core CPU \cite{chen:2013}. Operator level parallelism can potentially improve this.

\subsection{Node Selection}

The selection operator is less of a problem than in the related work, due to the parallel architecures
supporting recursion. Branches can recurse, using shared memory (faster memory available to a block of
threads), until an optimal or infeasibile solution is found along the branch. At this point however, the 
branch must return to global memory at which point the solution must be compared to the solutions of the other
branches. New threads are then created to search branches from nodes which have not been searched.

%\nocite{*}
\bibliographystyle{witseie}
\bibliography{sample}

\appendix

\clearpage 

\section{Parallel Algorithm Examples }

\section{Heuristic for Lower Bound Estimation}
\end{document}


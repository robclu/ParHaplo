%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% witseiepaper-2005.tex
%
%                       Ken Nixon (12 October 2005)
%
%                       Sample Paper for ELEN417/455 2005
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[10pt,twocolumn]{witseiepaper}

%
% All KJN's macros and goodies (some shameless borrowing from SPL)
\usepackage{KJN}
\usepackage{textcomp}
\usepackage{url}

\newcommand{\K}{\textit{k }}
\newcommand{\M}{\textit{m }}
\newcommand{\N}{\textit{n }}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF Info %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ifpdf
\pdfinfo{
/Title (Parallel Individual Haplotyping Assembly : Xeon Phi vs. Nvidia K20x )
/Author (Robert Clucas)
/CreationDate (D:200309251200)
/ModDate (D:200510121530)
/Subject (ELEN4002 Laboratory Project, 2015)
/Keywords (Haplotyping, GPU, Branch, Bound, Simplex)
}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE ETC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{ Parallelizing the Individual Haplotying Assembly Problem }

\author{Robert J. clucas
\thanks{School of Electrical \& Information Engineering, University of the
Witwatersrand, Private Bag 3, 2050, Johannesburg, South Africa}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\abstract{}

\keywords{Brach, Bound, GPU, Haplotyping, Simplex}

\maketitle
\thispagestyle{empty}\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{INTRODUCTION}

It is commonly accepted that all humans share $\mathtt{\sim}$99$\%$ of the same DNA, however, small variations 
cause human beings to have different physical traits. Single nucleotide polymorphisms (SNPs), which are
variations of a single DNA base from one individual to another, are believed to be able to address
genetic differences. For diploid organisms, which have pairs of chromosomes, a \textit{haplotype} is a 
sequence of SNPs in each copy of a pair of chromosomes. A \textit{genotype} describes the conflated data of the
haplotypes on a pair of chromosomes. Haplotypes are believed to contain more generic information than
genotypes \cite{stephens:2001}, however, obtaining haplotypes correctly is a difficult problem, which is 
broken into two subdomains: haplotype assembly and haplotype inference. \\
Haplotype inference uses the genotype of a set of individuals. The genotype data tells the status of each
allele at a position, but does not distinguish which copy of the chromosome the allele came from.
This negative aspects of this approach are that it cannot distinguish rare and novel SNPs \cite{he:2010}, 
and there is no way of knowing if the inferred haplotype is completely correct. \\
Individual haplotype assembly uses fragments of sequences generated by sequencing technology to determine
haplotypes. The fragments of a sequence come from the two copies of an individual's chromosome, the goal of the
individual haplotyping problem is to correctly determine two haplotypes, where each haplotype corresponds to
one of the two copies of the chromosome. \\
The haplotype assembly problem was proven to be NP-Hard \cite{lippert:2002}. The algorithms used to solve the
problem are thus computationally complex and until recently, there was no practical exact algorithm to solve 
the problem using minimum error correction (MEC) \cite{bonizzoni:2003},
However, recently an exact solution was proposed by \cite{chen:2013} which is capable of solving the MEC 
problem exactly, and can thus correctly infer all haplotypes from the fragment sequences. Due the NP-Hardness 
of the problem, the algorithm results in long run times - in the range of days for chromosomes with high
errors rates. Using a parallel implementation of any of the proposed solutions could reduce the long run
times, allowing useful haplotype information to be quickly inferred from the available datasets, having
positive effects in fields such as drug discovery, prediction of diseases, and variations in gene
expressions, to name a few. \\
Come back to introduction ... \\
Parallel programming makes use of devices which have many simple cores, but which can execute the same
instructions on each of the cores at the same time. The effectiveness of parallel programming is dependant on
the nature of the problem, as per Amdahl's law. The first attempts at parallel programming came from Graphics
Processing Units (GPUs) which were used to render many pixels simultaneously. More recently, General-Purpose
GPU (GPGPU) programming has become prominent with API's like CUDA and OpenCL, allowing access to GPUs from C and
C++ programs. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{HAPLOTYPE ASSEMBLY PROBLEM}

This section will provide a brief overview of the haplotype assembly (HA) problem, and define the
notation used through the rest of the paper. The input to the problem is a set of reads from a given genome
sequence, where each read contains fragments from each of the two chromosomes which make up the genome
sequence. These characters of a read consist of elements from a \textit{ternary string}, where a ternary
string has characters from the set \{0, 1, -\}. A value of 0 refers to the major allele
at a site, a value of 1 to the minor allele, and a value of - to the lack of a read at the site, and is
referred to as a \textit{gap}. These reads are then combined to form a matrix, M, where each row of the 
matrix corresponds to a read. \\
Each column of the matrix is known as an SNP site. At each site, the data could be accurate, missing, or have
error. The goal of the haplotype assembly problem is to determine a haplotype, H = \{\textit{h,h'}\}
from the matrix. The following terminology will be used to refer to properties of the matrix and the
fragments. \\
For the input matrix M, the number of fragments is denoted by \textit{m}, which is the number of rows in M.
The number of SNP sites is denoted by \textit{n}, which is the number of columns in M, while the j$^{th}$ 
site of the i$^{th}$ fragment is given by \textit{f$_{ij}$}. Furthermore, two fragments are said to 
conflict if the following conditions are true:
\begin{itemize}
    \item{f$_{ik}$ $\neq$ f$_{jk}$ \textbf{and} f$_{ik}$ $\neq$ '-' \textbf{and} f$_{jk}$ $\neq$ '-' }
\end{itemize}
Essentially this means that for two fragments i and j, if at an SNP site k, the reads do not have error and are 
not gaps, the reads have different values (fragment i has a value 0 at site k, while fragment j has a value 
1 at site k, or vice versa). \\
Following the notion of a conflict, the \textit{distance} between two fragments (or ternary strings) is
denoted by d( f$_i$, f$_j$ ) is the total number of positions for which the two fragments f$_i$ and f$_j$
conflict. Furthermore, to understand some of the problem formulations from the fragment data, it is useful to
define a \textit{conflict graph} \cite{lancia:2001} G = \{ V, E \}, where V corresponds to a fragment, and E
corresponds to and edge between two fragments if they conflict. If the matrix M contains no errors, then none
of the fragments from the same chromosome will conflict and G will be bipartate. Fragments from the same
chromosome may conflict. However, if there are errors (as is usually the case) in M, G will not be
bipartate. The haplotype assembly problem then requires the correction of G from a non-bipartate 
graph to a bipartate graph. There are numerous methods for solving the problem:

\begin{itemize}
    \item{ \textbf{Minimum Fragment Removal (MFR) :} This invloves removing the least number of fragments 
            from the input data such that the resultant graph G is bipartate. It is shown in 
            \cite{lancia:2001} that this can be solved in polynomial time.
        }
    \item{ \textbf{Weighted Minimum Edge Removal (WMER) \cite{aguiar:2012} :} This method is a recent method,
            which requires determining the minimum number of edges to remove such that removal of the edges
            results in G being bipartate.
        }
    \item{ \textbf{Longest Haplotype Reconstruction (LHR) :} This requires finding a set of fragments which,
            when they are removed from M, result in G being bipartate and the length of the resultant 
            haplotypes being maximized \cite{schwartz:2010}. 
        }
    \item{ \textbf{Minimum Error Correction (MEC) :} This method involves correcting the minimum number of 
            elements (sites for all fragments) in the matrix M which allow the graph G to be bipartate. 
            Although being the most complex method, it is the most widely used method as it provides the 
            highest accuracy rates. Only recently has an exact algorith for the MEC problem formulation been
            proposed which can provide an exact solution for all cases.
        }
\end{itemize}

Due to the accuracy and more extensive previous work, the MEC method for solving the problem was decided upon
as the method for which a parallel implementation will be proposed.

\subsection{ Minimum Error Correction Implementations } \label{sec:mecimp}

Much research has been done on solving the MEC formulation of the HA problem. The first exact algorithm for
solving the problem was a branch and bound algorithm proposed by \cite{wang:2005}. The algorithm creates a
tree which covers the search space of all possible corrections. The tree is then traversed to find the best
solution. They use an upper bound for when branching that allows branches to be abandoned when a better
solution than the current best cannot result from further exploration of the branch, thus improving
performance. However, this exact method has time complexity of O(2$^{\textit{m}}$), where \M is the
number of fragments in the input data, and hence cannot produce solutions for large problem sizes. They also 
provide a heuristic \textit{genetic algorithm} (GA) to improve the computational time, which only requires 
run times up to three orders of magnitude faster than the branch and bound implementation. While the GA
implementation gives very similar results to the branch and bound implementation, it is slightly worse. \\
A dynamic programming solution was proposed by \cite{xie:2008} which addresses the run time problem for large
input sizes. The algorithm has time complexity of O(\textit{mk}3$^{\textit{k}}$ + \textit{mlogm} +
\textit{mk}), where \K is the maximum number of SNP sites a fragment covers. In practice \K is usually small,
and results were shown small \K (less than 100). The proposed dynamic programming solution had significant run
time improvements over the solution proposed by \cite{wang:2005}. However, for larger \K values, the 
algorithm cannot solve the MEC case of the HA problem in a feasible amount of time. \\
More recently, \cite{chen:2013} proposed an exact algorithm for solving the MEC problem. The proposed
algorithm is the currently the only algorithm which can solve the HA problem for both the homozygous (the 
alleles at a site are identical) and hetrozygous (the alleles at an SNP site are different) case. Most other 
work assumes that the input fragment data is hetrozygous, which, while true for most of the SNP sites in the
input matrix, is normally false for a small number of the SNP sites. The exact solution removes unnecessary
data from the matrix, and partitions the matrix into smaller sub-matrices. The problem is then formulated as
an \textit{integer linear programming} (ILP) problem and solved as an optimization problem. The implementation was
run using an Intel i7-3960X CPU, and the HuRef dataset required 15 days to solve for the general case, and 24h
for the all-hetrozygous case. Heuristics methods are also proposed which speed up computation time by up to 15
times. However, the results are only shown for smaller input sizes, and the heuristic methods do not always
determine the optimal solution. Furthermore, the solutions for the general case show lower MEC scores, meaning
that the all-hetrozygous assumption is not always valid. 

\subsection{Parallelization}

Of the MEC implementations discussed in Section~\ref{sec:mecimp}, the branch and bound solution proposed by
\cite{wang:2005}, and the ILP formulation of the problem proposed by \cite{chen:2013} have the most potential 
for a parallel implementation. While there are other methods for solving ILP problems \cite{galli:2014} the 
most common methods for solving ILP problems are:
\begin{itemize}
    \item{ \textbf{Branch and bound :} The search space is divided into sub spaces, each of which is explored
            for the optimal solution. Bounds are enforced to ensure that sections of the sub spaces for which
        an optimal solution will not be found, will not be searched.
    }
    \item{ \textbf{Branch and cut :} The problem is first solved without the integer constraints to find the
        optimal solution. Cutting planes are then used to divide the search space and then branch and bound is
        applied.
    }        
\end{itemize}
Thus both the MEC methods mentioned above can be solved using a branch and bound algorithm. The branch and cut
algorithm involves using a branch and bound method as well as the simplex method, and is thus more complex,
hence the choice was made to use the branch and bound algorithm. 

\section{BRANCH AND BOUND}

The branch and bound algorithm can be parallelized in multiple ways \cite{crainic:2006}. Either specific,
computationally difficult functions can be accelerated by a parallel implementation, for example matrix
inversion, or the entire tree can be search in parallel, or a combination of both can be employed. Using tree
based approaches will require communication between each of the processes which searches a branch, as the
branch solution will need to be compared against the solutions of the other branches. This becomes a
bottleneck for both CPU and GPU implementations as groups of threads are assigned local memory, for which
access is much faster than the global memory space. There have been numerous parallel branch and bound methods
proposed for both the CPU and GPU which deal with these problems in different ways.

\subsection{ CPU Implementations } \label{ssec:cpuimp}

The ALPS framework \cite{xu:2005} is written in C++ and provides a parallel CPU implementation of the branch
and bound algorithm. The framework is tested on the knapsack problem \cite{kedia:2005}, which is an ILP problem 
and is known to be NP-hard. The speedup achieved is near linear in the number of nodes when the number of 
nodes is small, but diminishes as more nodes are added due to the amount of communication between the nodes. 
A speedup of 8 times is achieved for 8 nodes, and 26 times for 32 nodes. It is likely that for the HA problem, 
the number of nodes could be extremely large in which case this methods will be insufficient. \\
The MALLBA framework \cite{alba:2002} is also written in C++ and provides a parallel branch and bound
algorithm. It labels nodes as \textit{master} or \textit{slave} nodes, which defines the type of work that the
nodes do. The framework also provides heuristic methods to solve the problems more efficiently, however, the
accuracy of the results is less, which while acceptable for many applications, is not acceptable for the HA
problems. The performance results are similar to the ALPS framework, where near linear speed up is achieved for
a small number of additional nodes, but levels off for larger numbers of nodes.

\subsection{ GPU Implementations } \label{ssec:gpuimp}

A hetrogeneous CPU-GPU implementation was proposed by \cite{bouk:2012} and was applied to the knapsack 
problem. Due to the overhead of transferring data from the CPU to the GPU before computation, the model only 
uses GPUs when the tree has a large number of nodes ($>$ 5000), otherwise the CPUs are used Furthermore, the 
tree is built using a \textit{breadth first} strategy to favour the parallel nature of the GPUs. Both the
branching and bounding steps can be performed on the CPU or GPU. If the number of nodes is sufficiently large
such that full occupancy of the GPU is ensured, then for each iteration of the algorithm the GPU does the
branching and bounding on a list of nodes, eliminates nodes for which an optimal solution cannot be found, and
return the list back to the CPU. This process continues until convergence. This regular communication between
the CPU and GPU is expensive, which lessens the speedup achieved by the implementation. However, a speedup of 
up to 9.27 times was achieved for larger problem sizes, validating the feasibility of the branch and bound 
algorithm for parallel implementation. It must be noted that the algorithm was developed for Nvidia's Fermi
architecture, which does not support dynamic parallelism \cite{nvidia:2012}, hence results in the vast CPU-GPU 
communication. The newer Kepler architecture, however, does support dynamic parallelism. \\ 
In \cite{melab:2012, chakroun:2012}. algorithms are proposed which target the bounding operation for parallelism,
as well as dealing with the problem of thread divergence when using GPUs for branch and bound, which comes
from the position of the nodes in the tree, and the need to for branches to communicate with other branches to
compare solutions. Depending on the problem to which branch and bound is applied, the tree structure can be
irregular which makes the entire tree search unsuited for GPUs not supporting recursion and dynamic 
parallelism, as was the case when the algorithm was proposed - hence the focus on only he bounding 
operation. The algorithm was applied to the Flow-Shop scheduling problem, for which it is shown that
$\mathit{\sim}$98$\%$ of the computation is spent on the bound operation. Speed ups of up to
100 times were achieved by the GPU implementation over a multi-threaded CPU implementation. However, this 
kind of speedup will only be seen in problems where the calculation of the bounds is the bottleneck. 
Nevertheless, even if significantly less time is spent calculating the bounds, the speed up should still be
considerable.\\
The algorithms of \cite{melab:2012, chakroun:2012} are extended by \cite{chakroun:2013} to 
include not only parallelization of the bounding operator, but also the branching and pruning operators. This
essentially uses the GPU for doing all the searching of the subspaces. However, the CPU is still used for the
between each iteration as each GPU thread can only determine the solution of its first child nodes, rather 
than all child nodes until the leaves of the tree. Again this is due to the limitations of the Fermi 
architecture not supporting dynamic parallelism, despite this hardware limitation, the algorithm is still able
to achieve a speed up of up to 166 times over a multi-threaded CPU implementation, for large problem sizes.

\subsection{ Potential Improvements }

Sections~\ref{ssec:cpuimp} and \ref{ssec:gpuimp} above provide evidence of the feasibility of taking a
parallel approach to the branch and bound algorithms, and hence the feasibility of solving the HA problem with
a parallel implementation. Some aspects of the related work have significant performance implications on the
parallel implementation, such as:

\begin{itemize}
    \item{ Using multiple CPUs does not scale effectively, and hence the relative performance increase for
        adding an additional CPU decreases for each CPU which is added
    }
    \item{ The amount of communication between the CPU and the GPU is large, and is required on each
            iteration, which decreases the performance of the algorithm
    }
    \item{ The structure of the tree depends on the problem - one problem may ensure a balanced tree while
            another may map to an unbalanced tree - which makes a GPU difficult due to its data-parallel
            nature
    }
\end{itemize}

The above factors come from the limitations of the parallel hardware available at the time. Due to the lack of
recursive ability for the Fermi architecture used, each time solutions need to be compared for each of the
branches, and then to select the most relevant nodes from the list of searchable nodes, all results from the
GPU kernels needed to be passed back to the CPU since the GPU cannot invoke kernels itself. Furthermore, the 
problem of an unbalanced tree was significant for the same reason. However, the newer Kepler architecture does
allow kernels to call kernels, which should reduce the CPU-GPU communication and be able to handle unbalanced
trees as the kernel can itself determine the best searchable nodes. The Intel Xeon Phi also supports
recursion, and is thus also a good candidate for this approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\nocite{*}
\bibliographystyle{witseie}
\bibliography{sample}

%{\tiny \vfill \hfill \today \hspace{5mm} witseie-paper-2003.\TeX}

\end{document}

